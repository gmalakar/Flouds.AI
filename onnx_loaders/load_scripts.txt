## Export policy: see `export_policy.yaml` for preferred exporter per-model.
## If not listed in the manifest, default is v2 (main_export) and the exporter will
## attempt v1 only as a fallback. Use `--usev1` to force legacy ORTModel export.
##
## Task mapping quick reference:
## - Embeddings / feature extraction: `--model_for fe` and `--task feature-extraction` or `--use_t5_encoder` for T5 encoders
## - Seq2Seq (summarization/translation): `--model_for s2s` and `--task seq2seq-lm` (no KV-cache)
## - Seq2Seq with KV-cache (fast autoregressive decoding): use `--task text2text-generation-with-past` AND `--use_cache`
## - LLM / text generation: `--model_for llm` and `--task text-generation`

# Embedding models (preferred exporter in comments)
# v2: sentence-transformers/all-MiniLM-L6-v2
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/all-MiniLM-L6-v2" --optimize
# v2: sentence-transformers/paraphrase-MiniLM-L6-v2
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/paraphrase-MiniLM-L6-v2" --optimize
# v2: sentence-transformers/sentence-t5-base (use --use_t5_encoder for encoder-only)
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/sentence-t5-base" --optimize --use_t5_encoder
# Fine-tune T5 embeddings before export (alias --finetune supported)
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/sentence-t5-base" --finetune --use_t5_encoder --optimize
# v2: PleIAs/Pleias-Pico
python onnx_loaders/export_model.py --model_for "fe" --model_name "PleIAs/Pleias-Pico" --optimize
# v2: sentence-transformers/all-mpnet-base-v2
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/all-mpnet-base-v2" --optimize --framework pt
# v1: intfloat/e5-base-v2 (use --usev1)
python onnx_loaders/export_model.py --model_for "fe" --model_name "intfloat/e5-base-v2" --optimize --force

# Summarization models
# v2: t5-small
python onnx_loaders/export_model.py --model_for "s2s" --model_name "t5-small" --optimize --task "seq2seq-lm"
# v2: facebook/bart-large-cnn
python onnx_loaders/export_model.py --model_for "s2s" --model_name "facebook/bart-large-cnn" --task "text2text-generation-with-past" --use_cache --force
# Repack external_data models into single-file ONNX during verification
python onnx_loaders/export_model.py --model_for "s2s" --model_name "facebook/bart-large-cnn" --task "text2text-generation-with-past" --use_cache --pack_single_file
# v2: Falconsai/text_summarization
python onnx_loaders/export_model.py --model_for "s2s" --model_name "Falconsai/text_summarization" --optimize --task "seq2seq-lm" --model_folder "falconsai_text_summarization"
# v2: google/pegasus-cnn_dailymail
python onnx_loaders/export_model.py --model_for "s2s" --model_name "google/pegasus-cnn_dailymail" --optimize --task "text2text-generation-with-past" --use_cache


# Manual export examples (use consolidated CLI `export_model.py`)
# Embedding models (legacy v1 compatibility via --usev1)
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/all-MiniLM-L6-v2" --task "feature-extraction" --optimize --optimization_level 2 --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/paraphrase-MiniLM-L6-v2" --optimize --task "feature-extraction" --opset_version 17 --optimization_level 2 --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/paraphrase-MiniLM-L6-v2" --optimize --task "feature-extraction" --opset_version 18 --optimization_level 2 --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/sentence-t5-base" --optimize --task "feature-extraction" --opset_version 18 --optimization_level 2 --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "PleIAs/Pleias-Pico" --optimize --task "feature-extraction" --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "sentence-transformers/all-mpnet-base-v2"  --task "feature-extraction" --optimize --optimization_level 2 --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "intfloat/e5-base-v2" --task "feature-extraction" --optimize --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "hkunlp/instructor-xl" --task "feature-extraction" --opset_version 17 --optimize --usev1
python onnx_loaders/export_model.py --model_for "fe" --model_name "hkunlp/instructor-xl" --task "feature-extraction" --opset_version 18 --optimize --usev1
python onnx_loaders/export_model.py --model_for "llm" --model_name "distilgpt2" --task "text-generation" --optimize --force

# LLM models (legacy v1)
python onnx_loaders/export_model.py --model_for "llm" --model_name "microsoft/Phi-3-mini-4k-instruct" --task "text-generation" --opset_version 17 --optimize --usev1
python onnx_loaders/export_model.py --model_for "llm" --model_name "microsoft/Phi-3-mini-4k-instruct" --task "text-generation" --opset_version 18 --optimize --usev1
python onnx_loaders/export_model.py --model_for "llm" --model_name "distilgpt2" --task "text-generation" --optimize --usev1
# Summarization models (legacy v1)
python onnx_loaders/export_model.py --model_for "s2s" --model_name "t5-small" --optimize --task "text2text-generation" --opset_version 17 --usev1
python onnx_loaders/export_model.py --model_for "s2s" --model_name "t5-small" --optimize --task "text2text-generation" --opset_version 18 --usev1
python onnx_loaders/export_model.py --model_for "s2s" --model_name "facebook/bart-large-cnn" --optimize --task "text2text-generation-with-past" --opset_version 19 --usev1 --use_cache
python onnx_loaders/export_model.py --model_for "s2s" --model_name "Falconsai/text_summarization" --optimize --task "text2text-generation" --model_folder "falconsai_text_summarization" --usev1
python onnx_loaders/export_model.py --model_for "s2s" --model_name "google/pegasus-cnn_dailymail" --optimize --task "text2text-generation-with-past" --opset_version 18 --usev1
